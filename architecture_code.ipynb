{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sys module for system-related functionality\n",
    "import sys\n",
    "\n",
    "# Import the openai module for using OpenAI's API\n",
    "import openai\n",
    "\n",
    "# Import the streamlit library for creating web applications\n",
    "import streamlit as st\n",
    "\n",
    "# Import the numpy library for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Import cosine_similarity function from sklearn for calculating cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Import SentenceTransformer from sentence_transformers for sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Import google.generativeai module (if it exists; please verify the correct module)\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Import json module for working with JSON data\n",
    "import json\n",
    "\n",
    "# Import requests module for making HTTP requests\n",
    "import requests\n",
    "\n",
    "# Import re module for regular expressions\n",
    "import re\n",
    "\n",
    "# Import word_tokenize and stopwords from nltk for natural language processing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set the device to load the model onto (e.g., \"cuda\" for GPU)\n",
    "device = \"cuda\"\n",
    "\n",
    "# Load the pre-trained model and tokenizer from the Mistral-7B-Instruct-v0.1 checkpoint\n",
    "base_LLM = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "############ User input ############\n",
    "user_input = \"what is your knowledge cutoff?\"\n",
    "\n",
    "# Tokenize the user input\n",
    "encoded_user_input = tokenizer(user_input, return_tensors=\"pt\")\n",
    "\n",
    "# Move the model inputs to the specified device (e.g., GPU)\n",
    "model_inputs = encoded_user_input.to(device)\n",
    "\n",
    "# Move the entire model to the specified device (e.g., GPU)\n",
    "base_LLM.to(device)\n",
    "\n",
    "# Generate a response from the model using the provided code snippet\n",
    "output = base_LLM.generate(inputs=model_inputs, do_sample=True, max_new_tokens=2048)\n",
    "\n",
    "# Decode and print the generated response\n",
    "decoded_response = tokenizer.decode(output[0])\n",
    "print(decoded_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with main category names as keys\n",
    "categories_data = {\n",
    "    'sports_stats': 'Statistics and performance metrics of professional athletes and sports personalities',\n",
    "    'entertainment_stats': 'Statistical insights into the achievements and activities of personalities in the entertainment industry',\n",
    "    'current_events': 'Up-to-the-minute coverage of breaking news on global current events',\n",
    "    'entertainment_updates': 'Latest developments, news, and highlights about your favorite movies, TV shows, and celebrities',\n",
    "    'research_papers': 'Exploration of research papers and scholarly articles on various topics',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Search LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SentenceTransformer class from the sentence_transformers library\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Specify the pre-trained model name to be used\n",
    "model_HF_name = \"Sakil/sentence_similarity_semantic_search\"\n",
    "\n",
    "# Create an instance of the SentenceTransformer model using the specified pre-trained model\n",
    "model_HF = SentenceTransformer(model_HF_name)\n",
    "\n",
    "# Get the values from the 'categories_data' dictionary and convert them to a list\n",
    "# Then, encode the list of category values using the SentenceTransformer model and convert the result to a PyTorch tensor\n",
    "categories_data_embeddings_hf_ = model_HF.encode(list(categories_data.values()), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini MultiModel Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing google.generativeai as genai\n",
    "import google.generativeai as genai\n",
    "\n",
    "# setting the api key\n",
    "genai.configure(api_key=\"Your_GEMINI_API_KEY\")\n",
    "\n",
    "# Define a function to calculate the embeddings of the input text\n",
    "def model_MM(input_text):\n",
    "    categories_data_embeddings_MM_ = genai.embed_content(\n",
    "        model=\"models/embedding-001\",\n",
    "        content=input_text,\n",
    "        task_type=\"retrieval_document\",\n",
    "        title=\"Embedding of inputs\")\n",
    "    return categories_data_embeddings_MM_['embedding']\n",
    "\n",
    "# calculate embedding of sentence2 and categories_data\n",
    "categories_data_embeddings_MM_ = model_MM(list(categories_data.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_best_result(user_prompt, embedding_model_MM, embedding_model_hf, categories_data_embeddings_MM, categories_data_embeddings_hf):\n",
    "    # Convert user prompt to embedding vector\n",
    "    user_prompt_embedding_MM = embedding_model_MM([user_prompt])\n",
    "    user_prompt_embedding_hf = embedding_model_hf.encode([user_prompt], convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarity with categories data embeddings\n",
    "    similarity_scores_MM = cosine_similarity(user_prompt_embedding_MM, categories_data_embeddings_MM)\n",
    "    similarity_scores_hf = cosine_similarity(user_prompt_embedding_hf, categories_data_embeddings_hf)\n",
    "\n",
    "    # Find the index of the best result with the highest score for both models\n",
    "    best_result_index_MM = np.argmax(similarity_scores_MM)\n",
    "    best_result_index_hf = np.argmax(similarity_scores_hf)\n",
    "\n",
    "    # Get the best first result with the highest score for both models\n",
    "    best_result_MM = list(categories_data.keys())[best_result_index_MM]\n",
    "    best_result_hf = list(categories_data.keys())[best_result_index_hf]\n",
    "\n",
    "    return best_result_MM, best_result_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Categories for a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt\n",
    "user_prompt = \"\"\n",
    "\n",
    "# Find the best result with the highest score for both models\n",
    "best_result_MM, best_result_hf = find_best_result(user_prompt, model_MM, model_HF, categories_data_embeddings_MM_, categories_data_embeddings_hf_)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best result from MM model:\", best_result_MM)\n",
    "print(\"Best result from HF model:\", best_result_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArXiv API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'requests' module for making HTTP requests and the 're' module for regular expressions.\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Define a function named 'query_arxiv' that takes a search query, start position, and maximum results as parameters.\n",
    "def query_arxiv(search_query, start=0, max_results=2):\n",
    "    \n",
    "    # Define the base URL for the ArXiv API.\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    \n",
    "    # Create a dictionary 'query_params' with the specified parameters for the API query.\n",
    "    query_params = {\n",
    "        'search_query': f'all:{search_query}',  # Construct the search query parameter.\n",
    "        'start': start,  # Set the start position parameter.\n",
    "        'max_results': max_results  # Set the maximum results parameter.\n",
    "    }\n",
    "\n",
    "    # Make an HTTP GET request to the ArXiv API using the 'requests.get' function with the constructed URL and parameters.\n",
    "    response = requests.get(base_url, params=query_params)\n",
    "\n",
    "    # Use regular expression (regex) to extract content between '<summary>' tags in the API response.\n",
    "    summaries = re.findall('<summary>(.*?)</summary>', response.text, re.DOTALL)\n",
    "\n",
    "    # Replace newline and tab characters with spaces, and trim leading/trailing spaces for each summary.\n",
    "    # Join the modified summaries into a single string separated by spaces.\n",
    "    merged_summary = ' '.join(summary.replace('\\n', ' ').replace('\\t', ' ').strip() for summary in summaries)\n",
    "\n",
    "    # Return the merged and cleaned summary.\n",
    "    return merged_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # Tokenize, remove stopwords, and merge into a string\n",
    "    merged_words = ' '.join([word for word in word_tokenize(text) if word.lower() not in set(stopwords.words('english'))])\n",
    "\n",
    "    # Remove ? ! , . from the string\n",
    "    merged_words = re.sub('[\\?\\!\\,\\.\\']', '', merged_words)  # Missing 'import re' statement\n",
    "\n",
    "    # strip the string\n",
    "    merged_words = merged_words.strip()\n",
    "\n",
    "    # Print the remaining keywords\n",
    "    return merged_words\n",
    "\n",
    "# Example usage\n",
    "prompt = \"\"\n",
    "extract_keywords(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bing News API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def get_current_information(query):\n",
    "    # Hardcoded values for the URL and subscription key\n",
    "    search_url = \"https://api.bing.microsoft.com/v7.0/news/search\"  # actual search URL\n",
    "    subscription_key = \"YOUR_BING_API_KEY\"  # Replace with the actual subscription key\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(search_url, headers={\"Ocp-Apim-Subscription-Key\": subscription_key},\n",
    "                            params={\"q\": query, \"textDecorations\": True, \"textFormat\": \"HTML\"})\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Extract descriptions from the response\n",
    "    descriptions = [item.get('description', '') for item in json.loads(json.dumps(response.json())).get('value', [])]\n",
    "\n",
    "    # Clean and format the descriptions\n",
    "    cleaned_description = re.sub('<.*?>', '', '\\n'.join(descriptions))\n",
    "    cleaned_description = re.sub('\\.\\.\\.', '', cleaned_description)\n",
    "    cleaned_description = re.sub('&#39;', \"'\", cleaned_description)\n",
    "\n",
    "    return cleaned_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity with Gemini MultiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_result_MM(user_prompt, embedding_model_MM, categories_data_embeddings_MM):\n",
    "    # Convert user prompt to embedding vector\n",
    "    user_prompt_embedding_MM = embedding_model_MM([user_prompt])\n",
    "\n",
    "    # Calculate cosine similarity with categories data embeddings\n",
    "    similarity_scores_MM = cosine_similarity(user_prompt_embedding_MM, categories_data_embeddings_MM)\n",
    "\n",
    "    # Find the index of the best result with the highest score for both models\n",
    "    best_result_index_MM = np.argmax(similarity_scores_MM)\n",
    "\n",
    "    # Get the best first result with the highest score for both models\n",
    "    best_result_MM = list(categories_data.keys())[best_result_index_MM]\n",
    "\n",
    "    return best_result_MM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification with Gemini MultiModel and trigger relevant Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\n",
    "\n",
    "if find_best_result_MM(user_prompt, model_MM, categories_data_embeddings_MM_) == 'research_papers':\n",
    "    information = query_arxiv(user_prompt)\n",
    "elif find_best_result_MM(user_prompt, model_MM, categories_data_embeddings_MM_) == 'current_events':\n",
    "    information = get_current_information(extract_keywords(user_prompt))\n",
    "\n",
    "# Create a prompt template with the obtained information\n",
    "prompt_template = f'''\n",
    "{information}\n",
    "\n",
    "answer the following question in a detailed manner based on the above information\n",
    "{user_prompt}\n",
    "'''\n",
    "\n",
    "# Generate a response from the base language model using the provided prompt\n",
    "output = base_LLM.generate(inputs=model_inputs, do_sample=True, max_new_tokens=2048)\n",
    "\n",
    "# Decode and print the generated response\n",
    "decoded_response = tokenizer.decode(output[0])\n",
    "print(decoded_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bye Bye"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-kc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
